{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SummEval TrustScore Inference\n",
        "\n",
        "Run TrustScore on SummEval using vLLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers datasets vllm tqdm pyyaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Preprocess SummEval Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from scripts.load_summeval import load_summeval_with_sources\n",
        "import os\n",
        "\n",
        "# Load SummEval\n",
        "jsonl_path = \"datasets/raw/summeval/model_annotations.aligned.jsonl\"\n",
        "data = load_summeval_with_sources(jsonl_path, max_samples=100)\n",
        "print(f\"Loaded {len(data)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scripts.preprocess_summeval import preprocess_to_trustscore_format\n",
        "\n",
        "# Preprocess to TrustScore format\n",
        "output_path = preprocess_to_trustscore_format(max_samples=100)\n",
        "print(f\"Preprocessed data saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run TrustScore Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scripts.run_summeval_inference import run_summeval_inference\n",
        "\n",
        "# Run inference and save to Google Drive\n",
        "run_summeval_inference(\n",
        "    input_file=\"datasets/processed/summeval_trustscore_format.jsonl\",\n",
        "    output_file=\"results/summeval_trustscore_100samples.jsonl\",\n",
        "    max_samples=100,\n",
        "    batch_size=10,\n",
        "    use_vllm=True,\n",
        "    save_to_drive=True,  # Save results to Google Drive\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Load results from Drive or local\n",
        "results_path = \"/content/drive/MyDrive/TrustScore/summeval_trustscore_100samples.jsonl\"\n",
        "\n",
        "# Try Drive first, fallback to local\n",
        "try:\n",
        "    results = []\n",
        "    with open(results_path, 'r') as f:\n",
        "        for line in f:\n",
        "            results.append(json.loads(line))\n",
        "    print(f\"[Info] Loaded {len(results)} results from Google Drive\")\n",
        "except FileNotFoundError:\n",
        "    # Fallback to local\n",
        "    results = []\n",
        "    with open(\"results/summeval_trustscore_100samples.jsonl\", 'r') as f:\n",
        "        for line in f:\n",
        "            results.append(json.loads(line))\n",
        "    print(f\"[Info] Loaded {len(results)} results from local storage\")\n",
        "\n",
        "# Extract trust scores\n",
        "trust_scores = []\n",
        "for r in results:\n",
        "    if 'trustscore_output' in r:\n",
        "        try:\n",
        "            ts = r['trustscore_output']['summary']['trust_score']\n",
        "            trust_scores.append(ts)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "if trust_scores:\n",
        "    print(f\"\\nTrust Score Statistics:\")\n",
        "    print(f\"  Mean: {np.mean(trust_scores):.3f}\")\n",
        "    print(f\"  Std: {np.std(trust_scores):.3f}\")\n",
        "    print(f\"  Min: {np.min(trust_scores):.3f}\")\n",
        "    print(f\"  Max: {np.max(trust_scores):.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
